---
title: "Lion_SDM_Wallace"
author: "Mika Leslie"
date: "8/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(spocc)
library(spThin)
library(dismo)
library(rgeos)
library(ENMeval)

library(tidyverse)
# install.packages("rgbif")
library(rgbif)
# library(maps)
# install.packages("dismo")
# library(dismo)
library(maptools)
library(viridis)
# install.packages("scrubr")
library(scrubr)
library(raster)
library(DHARMa)

# download lion data (will take a while)
# gbif_lion <- occ_data(scientificName = "Panthera leo", 
#                        hasCoordinate = TRUE, 
#                        continent = "Africa")

#data downloaded from Wallace
lion <- read_csv("Panthera_leo_original_gbif.csv")
```

Record of analysis for *Panthera leo*.

### 1) Getting coordinates
```{r}
# get the columns that matter for mapping and cleaning the occurrence data:
myspecies_coords <- lion %>% 
  dplyr::select(name, longitude, latitude, occurrenceStatus, coordinateUncertaintyInMeters, institutionCode, references) %>% 
  filter(occurrenceStatus == "PRESENT")

head(myspecies_coords) # 7461 records

```

### 2) Clean Data
```{r}
# remove rows with duplicate coordinates
occs.dups <- duplicated(myspecies_coords[c('longitude', 'latitude')])
occs <- myspecies_coords[!occs.dups,] #4466 obs

# make sure latitude and longitude are numeric (sometimes they are characters)
occs$latitude <- as.numeric(occs$latitude)
occs$longitude <- as.numeric(occs$longitude)

# give all records a unique ID
occs$occID <- row.names(occs)

# removing unlikely, impresise, incommplete, and impossible coordinates
occs <- coord_incomplete(coord_imprecise(coord_impossible(coord_unlikely(occs))))
# 4444 obs
```

### 3) Process Occurrence Data

The following code recreates the polygon used to select occurrences to
keep in the analysis.

```{r}
#Polygon created to select within Africa
selCoords <- data.frame(x = c(16.9763, 30.1599, 42.8161, 53.1127, 44.3985, 30.6876, 5.9024, -18.1796, -21.5195, 1.3321, 16.9763), y = c(-36.1847, -34.3913, -13.681, 13.5546, 11.9722, 33.6672, 39.5866, 32.785, 4.8474, 0.4587, -36.1847))
selPoly <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(selCoords)), ID=1)))

#Check boundary extent - cropping to Africa for now 
(data("wrld_simpl"))
plot(wrld_simpl, 
     xlim = range(selCoords$x),
     ylim = range(selCoords$y),
     axes=TRUE, 
     col="light yellow"
)
box()
# add the points
points(selCoords$x, selCoords$y, col='orange', pch=20, cex=0.75)
# plot points again to add a border, for better visibility
points(selCoords$x, selCoords$y, col='red', cex=0.75)

occs.xy <- occs[c('longitude', 'latitude')] %>% 
  drop_na() #NA values need to be removed for the following function
sp::coordinates(occs.xy) <- ~ longitude + latitude

# overlay occurrence points over the polygon
intersect <- sp::over(occs.xy, selPoly)

# removes na values, so selects for points where they overlap the focal polygon
intersect.rowNums <- as.numeric(which(!(is.na(intersect))))
occs <- occs[intersect.rowNums, ] #4271 obs

#Check occurrence points
plot(wrld_simpl, 
     axes=TRUE, 
     col="light yellow"
)
box()
# add the points
points(occs$longitude, occs$latitude, col='orange', pch=20, cex=0.75)
# plot points again to add a border, for better visibility
points(occs$longitude, occs$latitude, col='red', cex=0.75)
#points are still showing up outside of Africa(?)

#try method 2
sp::coordinates(occs) <- ~ longitude + latitude
occs_crop <- crop(occs, selPoly) 
#4254 obs

plot(wrld_simpl, 
     axes=TRUE, 
     col="light yellow"
)
box()
# add the points
points(occs_crop$longitude, occs_crop$latitude, col='orange', pch=20, cex=0.75)
# plot points again to add a border, for better visibility
points(occs_crop$longitude, occs_crop$latitude, col='red', cex=0.75)

occs_crop <- as.data.frame(occs_crop)
```

### 4) Spatial Thin

Spatial thinning selected. Thin distance selected is 10 km.

```{r}
output <- spThin::thin(occs_crop, 'latitude', 'longitude', 'name', thin.par = 10, reps = 100, locs.thinned.list.return = TRUE, write.files = FALSE, verbose = FALSE)
```

Since spThin did 100 iterations, there are 100 different variations of
how it thinned your occurrence localities. As there is a stochastic
element in the algorithm, some iterations may include more localities
than the others, and we need to make sure we maximize the number of
localities we proceed with.

```{r}
# find the iteration that returns the max number of occurrences
maxThin <- which(sapply(output, nrow) == max(sapply(output, nrow)))

# if there's more than one max, pick the first one
maxThin <- output[[ifelse(length(maxThin) > 1, maxThin[1], maxThin)]] 

# subset occs to match only thinned occs
occs_crop <- occs_crop[as.numeric(rownames(maxThin)),]  
#1273 obs
```


### 5) Obtain Environmental Data

Using WorldClim
(<a href="http://www.worldclim.org/" class="uri">http://www.worldclim.org/</a>)
bioclimatic dataset at resolution of 2.5 arcmin.

```{r}
#Get WorldClim bioclimatic variable rasters
clim_data <- getData("worldclim", var="bio", res=2.5)

# extract environmental values at occ grid cells
locs.vals <- raster::extract(clim_data[[1]], occs_crop[, c('longitude', 'latitude')])

# remove occs without environmental values
occs_crop <- occs_crop[!is.na(locs.vals), ]  
#1270 obs

# crop to extent of Africa spatial polygon
envsCrop <- raster::crop(clim_data, selPoly)
```

### 6) Process Environmental Data

```{r}
# make SpatialPoints object for to create point buffer
occs.xy <- occs_crop[c('longitude', 'latitude')]
sp::coordinates(occs.xy) <- ~ longitude + latitude

#Buffer size of the study extent polygon defined as 0.5 degrees.
bgExt <- rgeos::gBuffer(occs.xy, width = 0.5)

# crop the environmental rasters by the background extent shape
envsBgCrop <- raster::crop(clim_data, bgExt)

# mask the background extent shape from the cropped raster
envsBgMsk <- raster::mask(envsBgCrop, bgExt)

# sample random background points
set.seed(0)
bg.xy <- dismo::randomPoints(envsBgMsk, 10000)

# convert matrix output to data frame
bg.xy <- as.data.frame(bg.xy)  
colnames(bg.xy) <- c("longitude", "latitude")
```

### 7) Partition Occurrence Data

Occurrence data is now partitioned for cross-validation, a method that
iteratively builds a model on all but one group and evaluates that model
on the left-out group.

For example, if the data is partitioned into 3 groups A, B, and C, a
model is first built with groups A and B and is evaluated on C. This is
repeated by building a model with B and C and evaluating on A, and so on
until all combinations are done.

```{r}
# need just lat/long values in two columns 
occs.xy <- occs_crop[c('longitude', 'latitude')]

# partition data by lat/long into four bins of ~equal numbers
group.data <- ENMeval::get.block(occ = occs.xy, bg = bg.xy)

# pull out the occurrence and background partition group numbers from the list
occs.grp <- group.data[[1]]
bg.grp <- group.data[[2]]

#Visualize spatial partition
evalplot.grps(pts = occs.xy, pts.grp = group.data$occs.grp, envs = envsCrop)+ 
        ggplot2::ggtitle("Spatial block partitions: occurrences")
```

### Extract data for Maxent GUI

```{r}
# extracting values
occ_extract <- raster::extract(envsCrop, occs.xy)
bg_extract <- raster::extract(envsCrop, bg.xy)

# creating data frame
occ_extract <- as.data.frame(occ_extract)
bg_extract <- as.data.frame(bg_extract)

# Creating data frame with species name, coordinates, and bioclim data 
lion_SWD <- data.frame(cbind(Species = "Panthera leo", occs_crop[2:3], occ_extract)) 
bg_SWD <- data.frame(cbind(Species = "background", bg.xy, bg_extract)) 

# export samples with data files
#write.csv(lion_SWD, "lion_SWD.csv", row.names = F)
#write.csv(bg_SWD, "bg_SWD.csv", row.names = F)

#Download predicted climate rasters
envsFuture <- raster::getData("CMIP5", var = "bio", res = 2.5, rcp = 85, model = "AC", year = 70)

# need names to match up to our preset-time climate data
names(envsFuture) <- names(clim_data)

# cropping to Africa polygon (not sure if we really need to, but might make it go faster)
envsFuture_crop <- raster::crop(envsFuture, selPoly)

# extracting values
occ_future_extract <- raster::extract(envsFuture_crop, occs.xy)

# save as data frame and add lat/long + species
occ_future_extract <- as.data.frame(occ_future_extract) 
occ_future_extract <- data.frame(cbind(Species = "Panthera leo", occs_crop[2:3], occ_future_extract))


# save csv
write.csv(occ_future_extract, "lion_future_SWD.csv", row.names = F)# extracting values
occ_future_extract <- raster::extract(future_data_crop, occs.xy)

# saving as data frame, add long/lat back in, re-order
occ_extract_future_final <- as.data.frame(occ_extract) %>% 
  mutate(latitude = occs.xy$latitude) %>% 
  mutate(longitude = occs.xy$longitude) %>% 
  dplyr::select(longitude, latitude, 1:19)
# save csv
write_csv(occ_extract_future_final, "lion_future_SWD.csv")
```


### 8) Build and Evaluate Niche Model

```{r}
# define the vector of regularization multipliers to test
rms <- seq(1, 3, 1)
# iterate model building over all chosen parameter settings
e <- ENMeval::ENMevaluate(occ = occs.xy, env = envsBgMsk, bg.coords = bg.xy,
                          RMvalues = rms, fc = c('L', 'LQ', 'H'), method = 'user', 
                          occ.grp = occs.grp, bg.grp = bg.grp, 
                          clamp = TRUE, algorithm = "maxnet")

# unpack the results data frame, the list of models, and the RasterStack of raw predictions
evalTbl <- e@results
evalMods <- e@models
names(evalMods) <- e@tune.settings$tune.args
evalPreds <- e@predictions

# compare a wider range of models that can use a wider variety of feature classes and regularization multipliers
e.mx <- ENMevaluate(occs = occs.xy, envs = envsBgMsk, bg.coords = bg.xy, method = 'user', 
                          occ.grp = occs.grp, bg.grp = bg.grp,
                    algorithm = 'maxnet', partitions = 'block', 
                    tune.args = list(fc = c("L","LQ","LQH","H"), rm = 1:5))

# unpack the results data frame, the list of models, and the RasterStack of raw predictions
eval.mxTbl <- e.mx@results

# overall results
res <- eval.results(e.mx)

# select model with lowest AICc score
opt.aicc <- res %>% filter(delta.AICc == 0)
#fc.H_rm.1

opt.seq <- res %>% 
  filter(or.10p.avg == min(or.10p.avg)) %>% 
  filter(auc.val.avg == max(auc.val.avg))

mod.seq <- eval.models(e.mx)[[opt.seq$tune.args]]
# Here are the non-zero coefficients in our model.
mod.seq$betas

plot(mod.seq, type = "cloglog")

```


```{r}
# view response curves for environmental variables with non-zero coefficients
plot(evalMods[["rm.1_fc.L"]], vars = c('bio03', 'bio04', 'bio05', 'bio06', 'bio08', 'bio09', 'bio10', 'bio12', 'bio15', 'bio18', 'bio19'), type = "cloglog")
```

```{r}
# view ENMeval results
ENMeval::evalplot.stats(e.mx, stats = "auc.val", "rm", "fc")
```

```{r}
# Select your model from the models list
mod <- e.mx[["fc.H_rm.1"]]
```

```{r}
# generate cloglog prediction
pred <- predictMaxnet(mod, envsBgMsk, type = 'cloglog', clamp = TRUE) 
```

```{r}
# get predicted values for occurrence grid cells
occPredVals <- raster::extract(pred, occs.xy)
# define minimum training presence threshold
thr <- thresh(occPredVals, "mtp")
# threshold model prediction
pred <- pred > thr
```

```{r}
# plot the model prediction
plot(pred)
```

### Project Niche Model

You selected to project your model. First define a polygon with the
coordinates you chose, then crop and mask your predictor rasters.
Finally, predict suitability values for these new raster cells based on
the model you selected.

```{r}
projCoords <- data.frame(x = c(12.778, 30.7955, 35.1901, 33.652, 35.0582, 31.6744, 32.1578, 25.7662, 19.2183, 16.8453, 15.3951, 14.9117, 12.8023, 12.778), y = c(-15.8286, -14.9388, -19.0978, -20.8736, -22.7502, -25.4377, -28.1053, -33.5133, -33.9883, -28.0035, -26.99, -22.2783, -17.9467, -15.8286))
projPoly <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(projCoords)), ID=1)))
```

### Project Niche Model to New Time

Now download the future climate variables chosen with *Wallace*, crop
and mask them by projPoly, and use the maxnet.predictRaster() function
to predict the values for the new time based on the model selected.

```{r}
envsFuture <- raster::getData("CMIP5", var = "bio", res = 2.5, rcp = 85, model = "CC", year = 70)

predsProj <- raster::crop(envsFuture, projPoly)
predsProj <- raster::mask(predsProj, projPoly)

# rename future climate variable names
names(predsProj) <- paste0('bio', sprintf("%02d", 1:19))
# select climate variables
predsProj <- raster::subset(predsProj, names(envs))
```

```{r}
# predict model
proj <- predictMaxnet(mod, predsProj, type = 'cloglog', clamp = TRUE)
```

```{r}
# plot the model prediction
plot(proj)
```



