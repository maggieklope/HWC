---
title: "Untitled"
author: "Maggie Klope"
date: "8/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rgbif)
library(maptools)
library(dismo)
library(rgeos)
library(viridis)
library(scrubr)
library(raster)
library(DHARMa)
library(spocc)
library(sf)
library(rgdal)
library(spData)

# load Wallace functions
source(system.file('shiny/funcs', 'functions.R', package = 'wallace'))

```

Maxent GUI Code:

Can run with 1) samples with data (SWD) where you extract the climate data for each occurrence and background point 

or 2) using lat and long and climate rasters


### 1) Download species occurence data and get coordinates

3 Options that get a different number of observations

#### a) downlad using Wallace's spocc:occ() function

```{r}

results <- spocc::occ(query = "Panthera leo", 
                      from = "gbif", 
                      limit = 15000, # max number of records
                      has_coords = TRUE)

# select just the coordinates
myspecies_coords <- results_lion%>%
  dplyr::select(longitude, latitude, occurrenceStatus, coordinateUncertaintyInMeters, institutionCode, references) %>%
  filter(occurrenceStatus == "PRESENT") # do we want to set occruanceStatus to just present?


```

#### b) can use occ_data function for GBIF

```{r}
# can use occ_data function for GBIF
results <- occ_data(scientificName = "Panthera leo",
                      hasCoordinate = TRUE,
                      continent = "Africa")

results_lion <- results$data %>%
  rename(latitude = decimalLatitude) %>%
  rename(longitude = decimalLongitude)

```


### 2) Clean Data

```{r}
# remove rows with duplicate coordinates
occs.dups <- duplicated(myspecies_coords[c('longitude', 'latitude')])
myspecies_coords <- myspecies_coords[!occs.dups,]

# make sure latitude and longitude are numeric (sometimes they are characters)
myspecies_coords$latitude <- as.numeric(myspecies_coords$latitude)
myspecies_coords$longitude <- as.numeric(myspecies_coords$longitude)

# give all records a unique ID
myspecies_coords$occID <- row.names(myspecies_coords)

# removing unlikely, impresise, incommplete, and impossible coordinates
occs <- coord_incomplete(coord_imprecise(coord_impossible(coord_unlikely(myspecies_coords))))


# save lat/long points as a csv if needed
# lion_cleaned <- coord_incomplete(coord_imprecise(coord_impossible(coord_unlikely(myspecies_coords))))%>%
#   rename(lat = latitude) %>% # renaming lat/long so it works with projection later on?
#   rename(long = longitude)
# lat_long <- lion_cleaned %>% 
#   dplyr::select(lat, long)
# 
# write_csv(lat_long, "lion_coords.csv")

```

### 4) crop data using Africa boundary

We decided to use a shapefile of Africa instead of selecting with a polygon on Wallace because it's easier and cleaner

```{r}
# load world data from spData package
world_sp = as(world, "Spatial")
world_sf = st_as_sf(world_sp, "sf")

# filter to just Africa (removing Madagascar for now)
africa <- world_sf %>% 
  filter(continent == "Africa") %>% 
  filter(name_long != "Madagascar") %>% 
  dplyr::select(name_long)

# merge all the country polygons
africa_2 <- st_union(africa)
# plot(africa_2)

# convert to a spatial polygon
africa_spatial <- as_Spatial(africa_2, cast = TRUE)

# creates new data frame with observation data lat/long points
occs.xy <- occs[c('longitude', 'latitude')]

# convert species data to spatial layer
coordinates(occs.xy) <- ~longitude+latitude
projection(occs.xy) <- CRS('+proj=longlat +datum=WGS84')

# double-check that polygon and points have the same projection
crs(africa_spatial)
crs(occs.xy)

# overlays those points over the polygon
intersect <- sp::over(occs.xy, africa_spatial)
# could also use 
# raster::crop(occs.xy, africa_spatial)

# removes na values, so selects for points where they overlap the focal polygon
intersect.rowNums <- as.numeric(which(!(is.na(intersect))))

# filters our occurrence data to those overlapping points
occs <- occs[intersect.rowNums, ]
occs$name <- "Pantheria leo"


```

#### a) In case you want to create a map figure to double check everything
```{r}
# world map example

(data("wrld_simpl"))

plot(wrld_simpl, 
     xlim = range(cleaned_2$long),
     ylim = range(cleaned_2$lat), 
     axes = TRUE, 
     col = "light yellow"
     )
box()
# add the points
points(cleaned_2$long, cleaned_2$lat, col = 'blue', pch = 20, cex = 0.75)
# plot points again to add a border, for better visibility
points(cleaned_2$long, cleaned_2$lat, col = 'red', cex = 0.75)

# can also crop to Africa
plot(wrld_simpl,
     xlim = range(selCoords$x), # setting limit to extent of Africa
     ylim = range(selCoords$y),
     axes = TRUE,
     col = "light yellow"
)
box()
# add the points
points(selCoords$x, selCoords$y, col='orange', pch=20, cex=0.75)
# plot points again to add a border, for better visibility
points(selCoords$x, selCoords$y, col='red', cex=0.75)
```


### 3) Spatial thin

Since spThin did 10 iterations, there are 10 different variations of
how it thinned your occurrence localities. As there is a stochastic
element in the algorithm, some iterations may include more localities
than the others, and we need to make sure we maximize the number of
localities we proceed with.

```{r}
# just doing 10 replicates for now, thinning to 10 km
output <- spThin::thin(occs, 'latitude', 'longitude', 'name', 
                       thin.par = 10, 
                       reps = 10, # default on Wallace is 100, but run time is long, so a low number is good for testing
                       locs.thinned.list.return = TRUE, 
                       write.files = FALSE, 
                       verbose = FALSE)

# find the iteration that returns the max number of occurrences
maxThin <- which(sapply(output, nrow) == max(sapply(output, nrow)))

# if there's more than one max, pick the first one
maxThin <- output[[ifelse(length(maxThin) > 1, maxThin[1], maxThin)]]  

# subset occs to match only thinned occs
occs <- occs[as.numeric(rownames(maxThin)),]0

# save occs to upload to Wallace if needed
occs_wallace <- occs %>% 
  dplyr::select(name, longitude, latitude) # change order to match
write_csv(occs_wallace, "thinned_data_for_Wallace.csv")

```

### 4) Create background points

We tested creating background points within 

1) one background point per pixel

```{r}

# 1 background point per pixel method
# need to load a bioclim raster in order to get 1 background point/cell
bioclim_data <- raster::getData("worldclim", var = "bio", res = 2.5) # downloading worldclim data
bg_raster <- bioclim_data [[1]] # selectiong bioclim 1


```

2) buffers around occurrence points 

```{r}

# point-buffer approach
# create point buffers of 50 km
buffer_points <- circles(occs, d = 50000, lonlat = TRUE)

# getting random points from within our polygons
bg_points <-  spsample(buffer_points@polygons, 1000, type = 'random', iter = 1000)

```

3) We also tried minimum convex polygon from Wallace, but we don't think it's the best method because it's such a large area

if you want to test how differnet number of background points impacts results

```{r}

# create function for steps required to extract background points
bg_points<- function(var){
        exbg.xy <- dismo::randomPoints(envsBgMsk, var) #create random points
        exbg.xy <- as.data.frame(exbg.xy) #convert to dataframe
        colnames(exbg.xy) <- c("longitude", "latitude")
        exbg_extract <- raster::extract(envsCrop, exbg.xy) # extract climate data
        exbg_extract <- as.data.frame(exbg_extract)
        exbg_SWD <- data.frame(cbind(Species = "background", exbg.xy, exbg_extract))
        #write csv
        write.csv(exbg_SWD, file = paste0("maxent_SWD/exbg_SWD", var, ".csv"), row.names = F)
        return(exbg_SWD)
}
vals <- c(750, 1000, 5000, 10000)
df_list <- lapply(vals, bg_points)

```

### 5) Load environmental data

```{r}
bioclim_data <- raster::getData("worldclim", var = "bio", res = 2.5) # downloading worldclim data

# cropping to extent of Africal spatial polygon
bioclim_crop <- raster::crop(bioclim_data, africa_spatial)

# double checking to see they line up
plot(bioclim_crop[[1]])
plot(buffer_points, add = TRUE)

# remove occs without environmental values
# extract environmental values at occ grid cells
locs.vals <- raster::extract(bioclim_data[[1]], occs[, c('longitude', 'latitude')])
# drop
occs_crop <- occs_crop[!is.na(locs.vals), ]


```

### Land Cover Data

Downloaded from: http://due.esrin.esa.int/page_globcover.php

Reclassified following 

```{r}

#import downloaded ESA GlobCover 2009 file
land <- 'GLOBCOVER_L4_200901_200912_V2.3.tif'
land = raster(land)

#crop to Africa polygon
land_crop <- crop(land, selPoly)

# create land cover class values matrix for reclassification (see GlobCover2009_Legend.xls in downloaded cover data for id values and corresponding cover labels)
mtrx <- rbind(c(0, 21, 1), c(22, 91, 2), c(92, 101, 3), c(102, 121, 4), c(122, 161, 3),
              c(162, 171, 2), c(172, 181, 3), c(182, 260, 5))
land_rcl <- reclassify(land_crop, mtrx)

#crop to same extent as other layers for maxent
land_rcl <- crop(land_rcl, bioclim_data, snap = "near")
land_rcl <- raster::resample(land_rcl, bio_new[[1]], method = "ngb")

#write raster
writeRaster(land_rcl, "maxent_SWD/bio_layers/bias_extent/esa_landcover.asc", format="ascii", overwrite = T)

```

### 6) SPECIES WITH DATA METHOD

Extract environmental data for occurence and background points for species with data format for use with Maxent Gui

```{r}
# need just lat/long values in two columns to used raster extract
occs.xy <- occs[c('longitude', 'latitude')]
bg.xy <- as.data.frame(bg_points)

# extracting values
occ_extract <- raster::extract(bioclim_crop, occs.xy)
bg_extract <- raster::extract(bioclim_crop, bg.xy)

# saving as data frames, adding in data
occ_extract_final <- as.data.frame(occ_extract) %>% 
  mutate(latitude = occs.xy$latitude) %>% # adding long/lat back in
  mutate(longitude = occs.xy$longitude) %>% 
  mutate(species = "Panthera leo") %>% # adding species column
  dplyr::select(species, longitude, latitude, 1:19) # using select() to re-order columns

bg_extract_final <- as.data.frame(bg_extract) %>% 
  mutate(x = bg.xy$x) %>% 
  mutate(y = bg.xy$y) %>% 
  mutate(species = "background") %>% 
  dplyr::select(species, x, y, 1:19) %>% 
  drop_na() # remove points with NA values

# save csvs
write_csv(occ_extract_final, "lion_occ_SWD.csv")
write_csv(bg_extract_final, "lion_bg_SWD.csv")

```


### LAT/LONG AND RASTER METHOD

```{r}

# Save occurrence point lat/long
lion_cleaned <- coord_incomplete(coord_imprecise(coord_impossible(coord_unlikely(myspecies_coords))))%>%
  rename(lat = latitude) %>% # renaming lat/long so it works with projection later on?
  rename(long = longitude)
lat_long <- lion_cleaned %>%
  dplyr::select(lat, long)

write_csv(lat_long, "lion_coords.csv")

# Download full raster layer for each bioclim variable for Maxent (may use in conjunction with bias layer)
bio <- c(1:19) #vector for each bioclim variable
for(i in bio){
        # Maxent reads either csv files or a directory -> need to export each bioclim raster separately
        writeRaster(bioclim_data[[i]], filename=paste0("maxent_SWD/bio_layers/bio", i, ".asc"),
                    overwrite=TRUE)
}

```


#### bias layer

```{r}




```



### 7) Download predicted climate rasters

```{r}

future_data <- getData('CMIP5',
                       var = "bio",
                       res = 2.5, 
                       rcp = 85, 
                       model = 'AC', 
                       year = 70)

# need names to match up to our preset-time climate data
names(future_data) <- names(bioclim_data)


# cropping to Africa polygon (not sure if we really need to, but might make it go faster)
future_data_crop <- raster::crop(future_data, africa_spatial)

```

### 8) Extracting predicted climate variables

- Do we only need to do the occurrence points?
- does it need the long/lat again? Or does it want x/y?

```{r}
# extracting values
occ_future_extract <- raster::extract(future_data_crop, occs.xy)

# saving as data frame, add long/lat back in, re-order
occ_extract_future_final <- as.data.frame(occ_extract) 
# %>% 
  # mutate(latitude = occs.xy$latitude) %>% 
  # mutate(longitude = occs.xy$longitude) %>% 
  # dplyr::select(longitude, latitude, 1:19)

# save csv
write_csv(occ_extract_future_final, "lion_future_SWD.csv")

```

At this point, the SWD data files can be used to make prediction in the Maxent GUI, or used for modeling in R

### 9) Maxent GUI results in R

```{r}


```


### 10) SDM modeling in R

```{r}
library(dismo)
install.packages("rJava")
library(rJava)

# withhold 20% of the data for testing the model
fold <- kfold(occ_extract_final, k = 5) # making 5 groups
lion_test <- occ_extract_final[fold == 1, ]
lion_train <- occ_extract_final[fold != 1, ]

# making x argument
predictions <- occ_extract_final %>% 
  dplyr::select(bio01:bio19)

# making p argument (just lat/long, )
occurrence <- occ_extract_final %>% 
  dplyr::select(longitude, latitude)

occurrence <- as.vector(occurrence)

# making a argument
# background <- 

#fit the maxent model
lion.model <- maxent(x = predictions, 
                     p = occurrence
                     )


```


